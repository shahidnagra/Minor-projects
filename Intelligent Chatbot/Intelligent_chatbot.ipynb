{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intelligent_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "15ppfH8H5r16",
        "Vw7GwvmkaLyx",
        "gqO71R77aU5Y"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15ppfH8H5r16"
      },
      "source": [
        "\n",
        "### Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvdaicJX5yBL",
        "outputId": "01c354b3-c8e7-4e80-e9be-e965a0fd1cba"
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import re, string, unicodedata\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import wikipedia as wk\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt') \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5UO1Py35zLJ"
      },
      "source": [
        "#load the dataset and convert it into lower case\n",
        "data = open('/content/HR.txt','r',errors = 'ignore')\n",
        "raw = data.read()\n",
        "raw = raw.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSzqXSwN54PX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "531c17f4-4749-48a2-dace-df05e7568a0a"
      },
      "source": [
        "raw[:1000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'human resource management is the process of recruiting, selecting, inducting employees, providing orientation, imparting training and development, appraising the performance of employees, deciding compensation and providing benefits, motivating employees, maintaining proper relations with employees and their trade unions, ensuring employees safety, welfare and healthy measures in compliance with labour laws of the land and finally following the orders / judgements of the concern high court and supreme court, if any.\\n\\n\\n\\n\\n\\nhuman resource management involves management functions like planning, organizing, directing and controlling\\nit involves procurement, development, maintenance of human resource\\nit helps to achieve individual, organizational and social objectives\\nhuman resource management is a multidisciplinary subject. it includes the study of management, psychology, communication, economics and sociology.\\nit involves team spirit and team work.\\nit is a continuous process.\\nhuman resourc'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw7GwvmkaLyx"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l-UBQyo7HXX"
      },
      "source": [
        "sent_tokens = nltk.sent_tokenize(raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ir9r-oq7tBe"
      },
      "source": [
        "def Normalize(text):\n",
        "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "    #word tokenization\n",
        "    word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "    \n",
        "    #remove ascii\n",
        "    new_words = []\n",
        "    for word in word_token:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    \n",
        "    #Remove tags\n",
        "    rmv = []\n",
        "    for w in new_words:\n",
        "        text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
        "        rmv.append(text)\n",
        "        \n",
        "    #pos tagging and lemmatization\n",
        "    tag_map = defaultdict(lambda : wn.NOUN)\n",
        "    tag_map['J'] = wn.ADJ\n",
        "    tag_map['V'] = wn.VERB\n",
        "    tag_map['R'] = wn.ADV\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    lemma_list = []\n",
        "    rmv = [i for i in rmv if i]\n",
        "    for token, tag in nltk.pos_tag(rmv):\n",
        "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "        lemma_list.append(lemma)\n",
        "    return lemma_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqO71R77aU5Y"
      },
      "source": [
        "### Generate chatbot response"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvoKZkEI7vzD"
      },
      "source": [
        "welcome_input = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "welcome_response = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def welcome(user_response):\n",
        "    for word in user_response.split():\n",
        "        if word.lower() in welcome_input:\n",
        "            return random.choice(welcome_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb0xs5uo7ztn"
      },
      "source": [
        "def generateResponse(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    #vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    vals = linear_kernel(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
        "        print(\"Checking Wikipedia\")\n",
        "        if user_response:\n",
        "            robo_response = wikipedia_data(user_response)\n",
        "            return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response#wikipedia search\n",
        "def wikipedia_data(input):\n",
        "    reg_ex = re.search('tell me about (.*)', input)\n",
        "    try:\n",
        "        if reg_ex:\n",
        "            topic = reg_ex.group(1)\n",
        "            wiki = wk.summary(topic, sentences = 3)\n",
        "            return wiki\n",
        "    except Exception as e:\n",
        "            print(\"No content has been found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djUya0g3718i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d08df42-2ef4-452e-9f6e-7dda566ab107"
      },
      "source": [
        "flag=True\n",
        "print(\"My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response not in ['bye','shutdown','exit', 'quit']):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"Chatterbot : You are welcome..\")\n",
        "        else:\n",
        "            if(welcome(user_response)!=None):\n",
        "                print(\"Chatterbot : \"+welcome(user_response))\n",
        "            else:\n",
        "                print(\"Chatterbot : \",end=\"\")\n",
        "                print(generateResponse(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"Chatterbot : Bye!!! \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\n",
            "hi\n",
            "Chatterbot : hi\n",
            "tell me about maths \n",
            "Chatterbot : Checking Wikipedia\n",
            "Mathematics (from Greek: μάθημα, máthēma, 'knowledge, study, learning') includes the study of such topics as quantity (number theory), structure (algebra), space (geometry), and change (analysis). It has no generally accepted definition.Mathematicians seek and use patterns to formulate new conjectures; they resolve the truth or falsity of such by mathematical proof. When mathematical structures are good models of real phenomena, mathematical reasoning can be used to provide insight or predictions about nature.\n",
            "What is maternity leave ?\n",
            "Chatterbot : maternity leave\n",
            "according to the section 5 of the maternity benefit act 1961, it is woman employee right to claim for maternity leave for 26 weeks for 2. such woman employee can apply maternity leave just eight weeks before the date of are expected delivery.\n",
            "What is career planning ?\n",
            "Chatterbot : socialization is a process of making employees to mingle up with everyone for team-spirit\n",
            "development - hr\n",
            "career planning and career development; process of establishing personal career objectives by employees and acting in a manner intended to bring them about.\n",
            "Tell me about mughal empire ?\n",
            "Chatterbot : Checking Wikipedia\n",
            "The Mughal Empire, Mogul or Moghul Empire, was an early modern empire in South Asia. For some two centuries, the empire stretched from the outer fringes of the Indus basin in the west, northern Afghanistan in the northwest, and Kashmir in the north, to the highlands of present-day Assam and Bangladesh in the east, and the uplands of the Deccan plateau in south India.The Mughal empire is conventionally said to have been founded in 1526 by Babur,  a warrior chieftain from what today is Uzbekistan, who employed aid from the neighbouring Safavid- and Ottoman empires,  to defeat the Sultan of Delhi, Ibrahim Lodhi, in the First Battle of Panipat, and to sweep down the plains of Upper India.  The Mughal imperial structure, however, is sometimes dated to 1600, to the rule of Babur's grandson, Akbar,   This imperial structure lasted until 1720, until shortly after the death of the last major emperor, Aurengzeb, during whose reign the empire also achieved its maximum geographical extent.\n",
            "Tell me about History of India \n",
            "Chatterbot : Checking Wikipedia\n",
            "According to consensus in modern genetics anatomically modern humans first arrived on the Indian subcontinent from Africa between 73,000 and 55,000 years ago.  However, the earliest known human remains in South Asia date to 30,000 years ago.  Settled life, which involves the transition from foraging to farming and pastoralism, began in South Asia around 7,000 BCE.  At the site of Mehrgarh, Balochistan, Pakistan, presence can be documented of the domestication of wheat and barley, rapidly followed by that of goats, sheep, and cattle.\n",
            "Tell me about Techtonic plates\n",
            "Chatterbot : Checking Wikipedia\n",
            "Plate tectonics (from the Late Latin: tectonicus, from the Ancient Greek: τεκτονικός, lit. 'pertaining to building') is a scientific theory describing the large-scale motion of the plates making up the Earth's lithosphere since tectonic processes began on Earth between 3.3 and 3.5 billion years ago. The model builds on the concept of continental drift, an idea developed during the first decades of the 20th century. The geoscientific community accepted plate-tectonic theory after seafloor spreading was validated in the late 1950s and early 1960s.\n",
            "Tell me about NLP\n",
            "Chatterbot : Checking Wikipedia\n",
            "No content has been found\n",
            "None\n",
            "Tell me about natural language processing\n",
            "Chatterbot : Checking Wikipedia\n",
            "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.  The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
            "Bye\n",
            "Chatterbot : Bye!!! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NluhWvJN749x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik-ZkdEYOo4x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJlMJoWQEwCJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnE1YdcO8e_A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NkowCEKE_h-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}